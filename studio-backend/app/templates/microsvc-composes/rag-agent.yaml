"{{endpoint}}":
  image: opea/agent:1.3
  container_name: "{{endpoint}}"
  volumes:
    - ./agent-tools/:/home/user/tools/
  ports:
    - "{{port_key}}:9095"
  ipc: host
  environment:
    ip_address: ${public_host_ip}
    no_proxy: ${no_proxy}
    http_proxy: ${http_proxy}
    https_proxy: ${https_proxy}
    llm_engine: "{{llmEngine}}" #tgi/vllm/openai *options
    strategy: "{{strategy}}" #rag_agent_llama *option
    recursion_limit: "{{recursionLimit}}" #integer value
    model: "{{modelName}}"
    temperature: "{{temperature}}"
    max_new_tokens: "{{maxNewToken}}"
    stream: false
    tools: /home/user/tools/worker_agent_tools.yaml
    require_human_feedback: false
    llm_endpoint_url: "http://${public_host_ip}:{{llm_port}}"
    RETRIEVAL_TOOL_URL: "http://${public_host_ip}:{{megasvc_endpoint_port}}/{{rag_name}}"
    port: 9095
    # dynamic variables
    OPENAI_API_KEY: "{{openaiApiKey}}" #if llm_engine is openai, if not will be NA or removed