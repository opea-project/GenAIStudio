"{{endpoint}}":
  image: opea/agent:1.3
  container_name: "{{endpoint}}"
  depends_on:
  __AGENT_ENDPOINTS__
  volumes:
    - ./agent-tools/:/home/user/tools/
  ports:
    - "{{port_key}}:9090"
  ipc: host
  environment:
    ip_address: ${public_host_ip}
    no_proxy: ${no_proxy}
    http_proxy: ${http_proxy}
    https_proxy: ${https_proxy}
    llm_engine: "{{llmEngine}}" #tgi/vllm/openai *options
    strategy: "{{strategy}}" #react_llama *option
    recursion_limit: "{{recursionLimit}}" #integer value
    model: "{{modelName}}"
    temperature: "{{temperature}}"
    max_new_tokens: "{{maxNewToken}}"
    stream: false
    tools: /home/user/tools/supervisor_agent_tools.yaml
    require_human_feedback: false
    llm_endpoint_url: "http://${public_host_ip}:{{llm_port}}"
    WORKER_AGENT_URL: "http://${public_host_ip}:{{rag_agent_port}}/v1/chat/completions"
    SQL_AGENT_URL: "http://${public_host_ip}:{{sql_agent_port}}/v1/chat/completions"
    port: 9090
    # dynamic variables
    OPENAI_API_KEY: "{{openaiApiKey}}" #if llm_engine is openai, if not will be NA or removed

