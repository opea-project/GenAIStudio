"{{endpoint}}":
  image: ${REGISTRY}/agent:${TAG}
  container_name: "{{endpoint}}"
  ports:
    - "{{port_key}}:9096"
  volumes:
    - ./agent-tools/:/home/user/tools/
  ipc: host
  environment:
    ip_address: ${public_host_ip}
    no_proxy: ${no_proxy}
    http_proxy: ${http_proxy}
    https_proxy: ${https_proxy}
    llm_engine: "{{llmEngine}}" #tgi/vllm/openai *options
    strategy: "{{strategy}}" #sql_agent_llama *option
    db_name: "{{db_name}}"
    db_path: "{{db_path}}"
    recursion_limit: "{{recursionLimit}}" #integer value
    model: "{{modelName}}"
    temperature: "{{temperature}}"
    max_new_tokens: "{{maxNewToken}}"
    stream: false
    require_human_feedback: false
    llm_endpoint_url: "http://${public_host_ip}:{{llm_port}}"
    port: 9096
    # dynamic variables
    OPENAI_API_KEY: "{{openaiApiKey}}" #if llm_engine is openai, if not will be NA or removed
