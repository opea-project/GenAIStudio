"{{endpoint}}":
  image: ${REGISTRY}/agent:${TAG}
  container_name: "{{endpoint}}"
  ports:
    - "{{port_key}}:9096"
  ipc: host
  environment:
    ip_address: ${public_host_ip}
    no_proxy: ${no_proxy}
    http_proxy: ${http_proxy}
    https_proxy: ${https_proxy}
    llm_engine: "{{llm_engine}}" #tgi/vllm/openai *options
    strategy: "{{strategy}}" #sql_agent_llama *option
    db_name: "{{db_name}}"
    db_path: "{{db_path}}"
    recursion_limit: "{{recursion_limit}}" #integer value
    model: "{{modelName}}"
    temperature: "{{temperature}}"
    max_new_tokens: "{{max_new_tokens}}"
    stream: false
    tools: /home/user/tools/worker_agent_tools.yaml
    require_human_feedback: false
    llm_endpoint_url: "http://${public_host_ip}:{{llm_port}}"
    port: 9096
    # dynamic variables
    OPENAI_API_KEY: "{{OPENAI_API_KEY}}" #if llm_engine is openai, if not will be NA or removed
